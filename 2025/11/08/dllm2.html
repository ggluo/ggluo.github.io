<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Disrete diffusion model - 2 | Guanxiong Luo</title> <meta name="author" content="Guanxiong Luo"> <meta name="description" content="A computational imaging scientist. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%86&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://ggluo.github.io/2025/11/08/dllm2.html"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Guanxiong Luo</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/index.html">Blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Disrete diffusion model - 2</h1> <p class="post-meta">November 8, 2025</p> <p class="post-tags"> <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/generative-models"> <i class="fas fa-hashtag fa-sm"></i> generative models</a>   <a href="/blog/tag/diffusion-models"> <i class="fas fa-hashtag fa-sm"></i> diffusion models</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>In this post, I’ll try to explain the core idea behind discrete diffusion models. Let’s define our setting:</p> <ul> <li> <strong>Data (\(x_0\)):</strong> A sequence of tokens \(x_0 = (x_0^{(1)}, x_0^{(2)}, \dots, x_0^{(L)})\).</li> <li> <strong>Vocabulary (\(V\)):</strong> Each token comes from a discrete vocabulary of size \(K\). For text, this is \(V = \{\text{"a"}, \text{"aardvark"}, \dots, \text{"zygote"}, \text{"[MASK]"}\}\).</li> <li> <strong>Representation:</strong> We can represent a single token \(x^{(i)}\) as a <strong>one-hot vector</strong> of size \(K\).</li> </ul> <p>The core idea is to replace the <em>addition of Gaussian noise</em> with <em>multiplication by a transition matrix</em>.</p> <hr> <p>The forward process \(q(x_t \vert x_{t-1})\) defines the probability of transitioning from a token at step \(t-1\) to a token at step \(t\). This is defined by a \(K \times K\) stochastic matrix \(Q_t\). The element \([Q_t]_{ij}\) represents the probability of a token \(i\) turning into a token \(j\) in one step.</p> <p>\begin{equation} q(x_t^{(i)} = j \vert x_{t-1}^{(i)} = i) = [Q_t]_{ij} \end{equation}</p> <p>If we use one-hot vectors for \(x_{t-1}\) (where it’s a row vector), this transition is a simple matrix multiplication:</p> <p>\begin{equation} q(x_t \vert x_{t-1}) = \text{Cat}(x_t ; p = x_{t-1} Q_t) \end{equation} This means the probability distribution for \(x_t\) is a Categorical distribution, with probabilities given by the vector \(x_{t-1} Q_t\). This matrix \(Q_t\) is <strong>not learned</strong>; it is a fixed schedule, just like the \(\beta_t\) schedule in continuous models. The key is in <em>how</em> we design \(Q_t\). Here are the two most common designs for text:</p> <h5 id="design-1-uniform-token-switching-diffusion">Design 1: Uniform (“Token-Switching”) Diffusion</h5> <p>This matrix says: at step \(t\), with probability \(1-\beta_t\) keep the token the same, and with probability \(\beta_t\), switch it to <em>any</em> token in the vocabulary (including itself) with uniform probability \(1/K\).</p> <p>Let \(\beta_t\) be the “noise” schedule. The matrix \(Q_t\) is:</p> <p>\(\) Q_t = (1 - \beta_t) \cdot I + \beta_t \cdot \frac{1}{K} \mathbb{1}\mathbb{1}^T \(\) * \(I\) is the \(K \times K\) identity matrix.</p> <ul> <li>\(\mathbb{1}\mathbb{1}^T\) is a \(K \times K\) matrix of all ones.</li> <li>\([Q_t]_{ij} = (1 - \beta_t)\) if \(i=j\), and \(\beta_t/K\) if \(i \neq j\). (Note: the diagonal is actually \((1-\beta_t) + \beta_t/K\))</li> </ul> <h5 id="design-2-absorbing-masking-diffusion">Design 2: Absorbing (“Masking”) Diffusion</h5> <p>This is the most common for text, as it’s analogous to BERT. We add a special <code class="language-plaintext highlighter-rouge">[MASK]</code> token to the vocabulary (let’s say it’s at index \(m\)).</p> <p>This matrix says: at step \(t\), with probability \(1-\beta_t\) keep the token the same, and with probability \(\beta_t\), <strong>replace it with the <code class="language-plaintext highlighter-rouge">[MASK]</code> token</strong>. Once a token becomes <code class="language-plaintext highlighter-rouge">[MASK]</code>, it can <em>never</em> change back.</p> <p>The matrix \(Q_t\) looks like this:</p> \[\begin{equation} [Q_t]_{ij} = \left\{ \begin{array}{ll} 1 &amp; \text{if } i = j = m \text{ ([MASK] stays [MASK])} \\ 1 - \beta_t &amp; \text{if } i = j \neq m \text{ (Token stays itself)} \\ \beta_t &amp; \text{if } j = m,\, i \neq m \text{ (Token becomes [MASK])} \\ 0 &amp; \text{otherwise ([MASK] never becomes a token)} \end{array} \right. \end{equation}\] <p>This is an “absorbing state,” which is why it’s called absorbing diffusion.</p> <p>Just like in continuous models, we want to jump from \(x_0\) to \(x_t\) in one step. Thanks to the Markov property, this is just matrix multiplication:</p> <p>\begin{equation} \bar{Q}_t = Q_1 Q_2 \dots Q_t \end{equation} Then, the probability of \(x_t\) given \(x_0\) is:</p> <p>\begin{equation} q(x_t \vert x_0) = \text{Cat}(x_t ; p = x_0 \bar{Q}_t) \end{equation} This \(\bar{Q}_t\) matrix is pre-calculated and fixed. The element \([\bar{Q}_t]_{ij}\) gives the total probability that an initial token \(i\) will have become token \(j\) after \(t\) steps of noising.</p> <p>This is the “denoising” step where the neural network comes in. We need to learn a model \(p_\theta\) that approximates the “ground truth” posterior \(q(x_{t-1} \vert x_t, x_0)\).</p> <p>Using Bayes’ theorem, we can derive this ground truth posterior:</p> <p>\begin{equation} q(x_{t-1} \vert x_t, x_0) \propto q(x_t \vert x_{t-1}, x_0) \cdot q(x_{t-1} \vert x_0) \end{equation} Because the forward process is Markovian, \(q(x_t \vert x_{t-1}, x_0) = q(x_t \vert x_{t-1})\).</p> <p>\begin{equation} q(x_{t-1} \vert x_t, x_0) \propto \underbrace{q(x_t \vert x_{t-1})}_{\text{from } Q_t} \cdot \underbrace{q(x_{t-1} \vert x_0)}_{\text{from} \bar{Q}_{t-1}} \end{equation}</p> <p>This gives us a (very messy) but <strong>fully computable</strong> categorical distribution for \(x_{t-1}\). In one-hot vector notation (with \(\odot\) as element-wise multiplication):</p> <p>\begin{equation} p = \underbrace{x_t (Q_t)^T}_{\text{Prob. of } x_t \text{ from } x_{t-1}} \odot \underbrace{x_0 \bar{Q}_{t-1}}_{\text{Prob. of } x_{t-1} \text{ from } x_0} \end{equation}</p> <p>\begin{equation} q(x_{t-1} \vert x_t, x_0) = \text{Cat}\left(x_{t-1} ; p’ = \frac{p}{\sum p}\right) \end{equation}</p> <p>This formula tells us, “Given the corrupted \(x_t\) and the original \(x_0\), what is the exact probability distribution for what \(x_{t-1}\) was?”</p> <p>The model \(p_\theta(x_{t-1} \vert x_t)\) is trained to match this ground truth distribution.</p> <ol> <li> <strong>Model \(\theta\):</strong> This is typically a Transformer. It takes \(x_t\) and the timestep \(t\) as input.</li> <li> <strong>Prediction:</strong> The Transformer’s job is to predict \(\hat{x}_0\) (the “clean” text). <ul> <li> \[\hat{p}_0 = \text{softmax}(\text{Transformer}(x_t, t))\] </li> <li> \[\hat{x}_0 = \text{one-hot}(\text{argmax}(\hat{p}_0))\] </li> </ul> </li> <li> <strong>Loss:</strong> The model \(p_\theta\) is formed by plugging this \(\hat{x}_0\) prediction into the Bayes formula from step 3.</li> </ol> <p>\begin{equation} p_\theta(x_{t-1} \vert x_t) \approx q(x_{t-1} \vert x_t, \hat{x}_0) \end{equation}</p> <p>The training loss is then a <strong>Cross-Entropy</strong> loss (or KL divergence) that forces this \(p_\theta\) distribution to match the true \(q(x_{t-1} \vert x_t, x_0)\) distribution.</p> <p>In simple terms: the Transformer (like BERT) is trained to look at a corrupted sentence (e.g., “The <code class="language-plaintext highlighter-rouge">[MASK]</code> brown fox”) and predict the original (“The quick brown fox”). This prediction is then used to calculate the “denoising” probability for the previous timestep.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2025/11/08/dllm.html">Disrete diffusion model - 1</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2025/11/06/pseudoinverse.html">Pseudoinverse Derivation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2023/03/11/learn-c-5.html">Learn C++ 5: Inline Function</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2023/03/09/learn-c-4.html">Learn C++ 4: std::array vs std::vector</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/2023/03/08/bindings-in-tensorrt.html">How Bindings Work with I/O in TensorRT</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Guanxiong Luo. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: November 08, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>