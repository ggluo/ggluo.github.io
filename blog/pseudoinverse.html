<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Pseudoinverse Derivation | Guanxiong Luo</title> <meta name="author" content="Guanxiong Luo"> <meta name="description" content="A computational imaging scientist. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%86&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://ggluo.github.io/blog/pseudoinverse"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Guanxiong Luo</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/index.html">Blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Pseudoinverse Derivation</h1> <p class="post-meta">November 6, 2025</p> <p class="post-tags"> <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/linear-algebra"> <i class="fas fa-hashtag fa-sm"></i> linear algebra</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <p>The concept of a pseudoinverse arises because we often encounter linear systems \(Ax = b\) that don’t have a unique, exact solution. The least-squares framework provides a way to find the “best” possible solution. This derivation is typically split into two main cases, both of which fall under the umbrella of “least-squares.”</p> <hr> <h4 id="1-the-overdetermined-case-tall-matrix-m--n">1. The overdetermined case (tall matrix, \(m &gt; n\))</h4> <p>This is the classic least-squares problem. We have more equations (\(m\)) than unknowns (\(n\)), and the system is likely inconsistent (meaning no \(x\) perfectly satisfies \(Ax = b\)).</p> <p><strong>The Goal:</strong> Find the vector \(x\) that minimizes the squared error, or the L2-norm of the residual vector \(r = Ax - b\). \begin{equation} x^* = \arg \min_{x} |Ax - b|^2 \end{equation}</p> <p>Let’s define the squared error \(E(x)\) as a function of \(x\). Using the dot product, the squared norm is \(r^T r\): \begin{equation} E(x) = (Ax - b)^T (Ax - b) \end{equation} We expand the terms:</p> \[\begin{aligned} E(x)&amp;= (x^T A^T - b^T) (Ax - b) \\ E(x)&amp;= x^T A^T A x - x^T A^T b - b^T A x + b^T b \end{aligned}\] <p>Notice that \(x^T A^T b\) and \(b^T A x\) are both scalars. In fact, they are transposes of each other, and since the transpose of a scalar is just the scalar itself, they are equal: \((b^T A x)^T = x^T A^T b\). So, we can combine them: \begin{align} E(x) = x^T A^T A x - 2 x^T A^T b + b^T b \end{align}</p> <p>To find the \(x\) that minimizes this error, we take the gradient of \(E(x)\) with respect to \(x\) and set it to zero. Using the matrix calculus rules:</p> <ul> <li>\(\nabla_x (x^T B x) = (B + B^T) x\) (which is \(2Bx\) if \(B\) is symmetric)</li> <li>\(\nabla_x (c^T x) = c\) (or \(\nabla_x (x^T c) = c\))</li> </ul> <p>The matrix \(A^T A\) is symmetric. Applying these rules, we get:</p> <p>\begin{equation} \nabla_x E(x) = 2(A^T A) x - 2(A^T b) + 0 \end{equation}</p> <p>Set the gradient to zero to find the minimum: \(\begin{align} 2(A^T A) x - 2(A^T b) =&amp; 0 \\ (A^T A) x =&amp; A^T b \end{align}\)</p> <p>This final equation, \(A^T A x = A^T b\), is known as the <strong>normal equation</strong>. To solve for \(x\), we can multiply by the inverse of \((A^T A)\) and get \(x = (A^T A)^{-1} A^T b\). This step requires a critical assumption: <strong>\((A^T A)\) must be invertible</strong>. This is true if and only if \(A\) has <strong>linearly independent columns</strong> (i.e., \(A\) has full column rank).</p> <p>We are looking for a matrix \(A^+\) (the pseudoinverse) such that \(x = A^+ b\). By direct comparison, we find the <strong>left pseudoinverse</strong>:</p> <p>\begin{equation} A^+ = (A^T A)^{-1} A^T \end{equation}</p> <p>It’s called the “left” inverse because if you multiply \(A\) on the left by \(A^+\), you get the identity matrix: \(A^+ A = \left[ (A^T A)^{-1} A^T \right] A = (A^T A)^{-1} (A^T A) = I\).</p> <hr> <h4 id="2-the-underdetermined-case-wide-matrix-n--m">2. The underdetermined case (wide matrix, \(n &gt; m\))</h4> <p>This is a different kind of “least-squares” problem. We have fewer equations (\(m\)) than unknowns (\(n\)), so the system $Ax = b$ has infinitely many solutions.</p> <p><strong>The Goal:</strong> Of all the possible solutions, find the <em>unique</em> solution $x$ that has the <strong>minimum norm</strong> (\(\min \|x\|^2\)). This is the “least-squares” solution in the sense that it’s the “smallest” solution vector. We want to solve the constrained optimization problem:</p> <p>\begin{equation} \min |x|^2 \quad \text{subject to} \quad Ax = b \end{equation}</p> <p>We can solve this using the method of Lagrange multipliers. The Lagrangian function \(L\) is:</p> <p>\begin{equation} L(x, \lambda) = x^T x + \lambda^T (Ax - b) \end{equation}</p> <p>Here, \(x^T x\) is \(\|x\|^2\) and \(\lambda\) is a vector of Lagrange multipliers. We find the minimum by setting the gradients with respect to both \(x\) and \(\lambda\) to zero.</p> <p><strong>Gradient w.r.t. \(x\):</strong></p> \[\begin{aligned} \nabla_x L &amp;= 2x + A^T \lambda = 0\\ x &amp;= -\frac{1}{2} A^T \lambda \end{aligned}\] <p><strong>Gradient w.r.t. \(\lambda\):</strong></p> \[\begin{aligned} \nabla_\lambda L &amp;= Ax - b = 0\\ Ax &amp;= b \end{aligned}\] <p>Now we combine these two results. Substitute the expression for \(x\) into the constraint equation:</p> \[\begin{aligned} A \left( -\frac{1}{2} A^T \lambda \right) &amp;= b\\ -\frac{1}{2} (A A^T) \lambda &amp;= b \end{aligned}\] <p>Now, solve for the multiplier vector: \(\lambda = -2 (A A^T)^{-1} b\)</p> <p>This step requires the assumption that <strong>\((A A^T)\) is invertible</strong>, which is true if and only if $A$ has <strong>linearly independent rows</strong> (i.e., \(A\) has full row rank).</p> <hr> <p>The full <strong>Moore-Penrose pseudoinverse</strong> (often derived using Singular Value Decomposition, or SVD) generalizes both of these cases. It also works for matrices that are <em>rank-deficient</em> (i.e., neither full column nor full row rank), which is when \((A^T A)\) or \((A A^T)\) would be singular (non-invertible).</p> <p>However, the “least-squares” perspective naturally gives rise to these two fundamental forms:</p> <table> <thead> <tr> <th style="text-align: left">Case</th> <th style="text-align: left">Matrix Shape</th> <th style="text-align: left">Problem</th> <th style="text-align: left">Solution \(x = A^+ b\)</th> <th style="text-align: left">Pseudoinverse \(A^+\)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Overdetermined</strong></td> <td style="text-align: left">Tall (\(m &gt; n\)), full column rank</td> <td style="text-align: left">\(\min |Ax - b|^2\)</td> <td style="text-align: left">Least-Squares Solution</td> <td style="text-align: left">\(A^+ = (A^T A)^{-1} A^T\)</td> </tr> <tr> <td style="text-align: left"><strong>Underdetermined</strong></td> <td style="text-align: left">Wide (\(n &gt; m\)), full row rank</td> <td style="text-align: left">\(\min |x|^2\) s.t. \(Ax=b\)</td> <td style="text-align: left">Minimum Norm Solution</td> <td style="text-align: left">\(A^+ = A^T (A A^T)^{-1}\)</td> </tr> </tbody> </table> <hr> <h4 id="3-pseudoinverse-using-singular-value-decomposition">3. Pseudoinverse using singular value decomposition</h4> <p>Here is the derivation of the pseudoinverse using the Singular Value Decomposition (SVD). This is the most general and powerful approach, as it works for <strong>all</strong> matrices, including those that are rank-deficient (where the previous least-squares derivations would fail).</p> <p>First, any \(m \times n\) matrix \(A\) can be decomposed using SVD as:</p> <p>\begin{equation} A = U \Sigma V^T \end{equation}</p> <p>Where:</p> <ul> <li> <strong>\(U\)</strong>: An \(m \times m\) orthogonal matrix (\(U^T U = I_m\)). Its columns are the <em>left singular vectors</em>.</li> <li> <strong>\(V\)</strong>: An \(n \times n\) orthogonal matrix (\(V^T V = I_n\)). Its columns are the <em>right singular vectors</em>.</li> <li> <strong>\(\Sigma\)</strong>: An \(m \times n\) diagonal matrix containing the singular values \(\sigma_1, \sigma_2, ...\) in decreasing order. These values are non-negative.</li> </ul> <p>If \(A\) has rank \(r\), then \(r\) of the singular values are positive, and the rest are zero.</p> <p>The key to the SVD approach is defining the pseudoinverse of the simple diagonal matrix \(\Sigma\).</p> <p>Let’s say \(\Sigma\) (an \(m \times n\) matrix) looks like this:</p> \[\Sigma = \begin{bmatrix} \sigma_1 &amp; 0 &amp; \dots &amp; 0 \\ 0 &amp; \sigma_2 &amp; \dots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \dots &amp; \sigma_n \\ \vdots &amp; \vdots &amp; \dots &amp; \vdots \\ 0 &amp; 0 &amp; \dots &amp; 0 \end{bmatrix}\] <p>To find its pseudoinverse \(\Sigma^+\), we do two things:</p> <ol> <li> <strong>Transpose</strong> its dimensions to get an \(n \times m\) matrix.</li> <li> <strong>Reciprocate</strong> all the <strong>non-zero</strong> singular values, leaving the zeros as zeros.</li> </ol> <p>If \(\Sigma_{ii} = \sigma_i &gt; 0\), then \((\Sigma^+)_{ii} = 1/\sigma_i\). If \(\Sigma_{ii} = 0\), then \((\Sigma^+)_{ii} = 0\).</p> <p><strong>Example:</strong></p> <p>If</p> \[\Sigma = \begin{bmatrix} \sigma_1 &amp; 0 &amp; 0 \\ 0 &amp; \sigma_2 &amp; 0 \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{bmatrix}\] <p>then</p> \[\Sigma^+ = \begin{bmatrix} 1/\sigma_1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1/\sigma_2 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix}\] <p>With \(\Sigma^+\) defined, the pseudoinverse of \(A\) (called the <strong>Moore-Penrose Pseudoinverse</strong>) is simply:</p> <p>\begin{equation} A^+ = V \Sigma^+ U^T \end{equation}</p> <p>This definition elegantly handles all cases: overdetermined, underdetermined, and rank-deficient. Now, let’s show that this SVD definition is consistent with the two cases we derived earlier.</p> <h5 id="case-1-overdetermined-tall-matrix-m--n-full-column-rank-rn">Case 1: overdetermined (tall matrix, \(m &gt; n\), full column rank \(r=n\))</h5> <ul> <li>Our previous formula was \(A^+ = (A^T A)^{-1} A^T\).</li> <li>Let’s plug the SVD into this formula and see if we get \(V \Sigma^+ U^T\).</li> </ul> <ol> <li> <p><strong>\(A^T A\)</strong>: \(A^T A = (U \Sigma V^T)^T (U \Sigma V^T) = (V \Sigma^T U^T) (U \Sigma V^T)\) Since \(U^T U = I\), this simplifies to: \(A^T A = V (\Sigma^T \Sigma) V^T\)</p> </li> <li> <p><strong>\((A^T A)^{-1}\)</strong>: \((A^T A)^{-1} = (V (\Sigma^T \Sigma) V^T)^{-1}\) Using the rule \((XYZ)^{-1} = Z^{-1} Y^{-1} X^{-1}\) and knowing \(V^{-1} = V^T\): \((A^T A)^{-1} = (V^T)^{-1} (\Sigma^T \Sigma)^{-1} V^{-1} = V (\Sigma^T \Sigma)^{-1} V^T\)</p> </li> <li> <p><strong>\(A^+ = (A^T A)^{-1} A^T\)</strong>: \(A^+ = \left[ V (\Sigma^T \Sigma)^{-1} V^T \right] (U \Sigma V^T)^T\) \(A^+ = \left[ V (\Sigma^T \Sigma)^{-1} V^T \right] (V \Sigma^T U^T)\) Since \(V^T V = I\), this simplifies to: \(A^+ = V (\Sigma^T \Sigma)^{-1} \Sigma^T U^T\)</p> </li> <li>analyze the \(\Sigma\) part: <ul> <li>\(\Sigma\) is \(m \times n\). Let’s say \(\Sigma = \begin{bmatrix} D_n \\ 0 \end{bmatrix}\), where \(D_n\) is an \(n \times n\) diagonal matrix of \(\sigma_i\).</li> <li>\(\Sigma^T = \begin{bmatrix} D_n &amp; 0 \end{bmatrix}\).</li> <li>\(\Sigma^T \Sigma = \begin{bmatrix} D_n &amp; 0 \end{bmatrix} \begin{bmatrix} D_n \\ 0 \end{bmatrix} = D_n^2\). (This is an \(n \times n\) invertible matrix).</li> <li>\((\Sigma^T \Sigma)^{-1} = (D_n^2)^{-1} = D_n^{-2}\).</li> <li>Now, \((\Sigma^T \Sigma)^{-1} \Sigma^T = D_n^{-2} \begin{bmatrix} D_n &amp; 0 \end{bmatrix} = \begin{bmatrix} D_n^{-1} &amp; 0 \end{bmatrix}\).</li> <li>This resulting matrix, \(\begin{bmatrix} D_n^{-1} &amp; 0 \end{bmatrix}\), is <strong>exactly the SVD definition of \(\Sigma^+\)</strong>.</li> </ul> </li> <li>Conclusion: \(A^+ = V (\Sigma^+) U^T\). The formulas match.</li> </ol> <h5 id="case-2-underdetermined-wide-matrix-n--m-full-row-rank-rm">Case 2: underdetermined (wide matrix, \(n &gt; m\), full row rank \(r=m\))</h5> <ul> <li>Our previous formula was \(A^+ = A^T (A A^T)^{-1}\).</li> <li> <p>A similar substitution (which I’ll skip the full algebra for) shows: \(A^+ = (V \Sigma^T U^T) \left[ (U \Sigma V^T)(V \Sigma^T U^T) \right]^{-1}\) \(A^+ = (V \Sigma^T U^T) \left[ U (\Sigma \Sigma^T) U^T \right]^{-1}\) \(A^+ = (V \Sigma^T U^T) \left[ U (\Sigma \Sigma^T)^{-1} U^T \right]\) \(A^+ = V \Sigma^T (\Sigma \Sigma^T)^{-1} U^T\)</p> </li> <li>Again, if you analyze the \(\Sigma\) part (\(\Sigma^T (\Sigma \Sigma^T)^{-1}\)), you’ll find it simplifies to \(\Sigma^+\).</li> <li> <strong>Conclusion</strong>: \(A^+ = V \Sigma^+ U^T\). The formulas match again.</li> </ul> <h5 id="why-the-svd-method-is-better">Why the SVD Method is Better</h5> <p>The SVD derivation \(A^+ = V \Sigma^+ U^T\) is superior because it <strong>never fails</strong>.</p> <ul> <li>In the rank-deficient case, \(r &lt; \min(m, n)\).</li> <li>This means both \((A^T A)\) and \((A A^T)\) will be singular (non-invertible) because their \(\Sigma\) components (\(D_r^2\)) will contain zeros. Our first derivation using \((...)^{-1}\) would fail.</li> <li>However, the SVD definition of \(\Sigma^+\) <strong>gracefully handles this</strong>. It only reciprocates the \(\sigma_i\) that are <em>non-zero</em> and leaves the zeros as zeros.</li> </ul> <p>This \(A^+ = V \Sigma^+ U^T\) always gives the unique <strong>minimum-norm least-squares solution</strong>:</p> <ol> <li> <strong>Least-Squares</strong>: It finds the \(x\) (or \(x\)’s) that minimizes the error \(\|Ax - b\|^2\).</li> <li> <strong>Minimum-Norm</strong>: If there are multiple solutions (as in the underdetermined or rank-deficient case), it picks the <em>one and only one</em> solution \(x\) that also has the smallest length \(\|x\|^2\).</li> </ol> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/softmax">Optimizing Softmax on GPU: From Mathematical Properties to Practical Implementation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/flash-attention-1">The details of flash attention - algorithm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/dllm2">Discrete diffusion model - 2</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/dllm">Discrete diffusion model - 1</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/learn-c-5">Learn C++ 5: Inline Function</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Guanxiong Luo. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: December 31, 2025. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-L1ZL85W5Y4"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-L1ZL85W5Y4");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>