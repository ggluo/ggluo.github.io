<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Optimizing softmax on GPU | Guanxiong Luo</title> <meta name="author" content="Guanxiong Luo"> <meta name="description" content="A computational imaging scientist. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%90%86&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://ggluo.github.io/blog/softmax"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">Guanxiong Luo</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/index.html">Blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Optimizing softmax on GPU</h1> <p class="post-meta">December 29, 2025</p> <p class="post-tags"> <a href="/blog/2025"> <i class="fas fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/cuda"> <i class="fas fa-hashtag fa-sm"></i> cuda</a>   <a href="/blog/tag/self-attention"> <i class="fas fa-hashtag fa-sm"></i> self-attention</a>   <a href="/blog/tag/numerical-computation"> <i class="fas fa-hashtag fa-sm"></i> numerical computation</a>   <a href="/blog/tag/coding"> <i class="fas fa-hashtag fa-sm"></i> coding</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h3 id="introduction">Introduction</h3> <p>Softmax is a fundamental operation in deep learning, particularly in attention mechanisms of transformer models. While mathematically simple, its efficient implementation on GPU hardware presents interesting challenges and optimization opportunities. In this blog post, we’ll explore the mathematical properties of softmax, implement multiple GPU kernels with increasing sophistication, and analyze their performance characteristics. Complete code is on <a href="https://github.com/ggluo/Minimal_Softmax" rel="external nofollow noopener" target="_blank">GitHub Repository</a>.</p> <h3 id="softmax-translation-invariance">Softmax: Translation Invariance</h3> <p>For a vector \(\mathbf{x} = [x_1, x_2, ..., x_n]\), the softmax function is defined as:</p> \[\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}\] <p>A key property of softmax is its translation invariance:</p> \[\text{softmax}(\mathbf{x} - c) = \text{softmax}(\mathbf{x})\] <p>where \(c\) is any constant. This property is crucial for numerical stability, as we can set #$c = \max(\mathbf{x})#$ to avoid overflow in exponential calculations—this is known as “safe softmax.”</p> <p>This property has a important insight: softmax can be computed incrementally using a reduction approach. If we partition a vector into two intervals \(I_1\) and \(I_2\), we can compute:</p> <p>Let \(m_1 = \max(I_1)\), \(m_2 = \max(I_2)\), and \(m = \max(m_1, m_2)\).</p> <p>Then for the exponential sums:</p> \[l = l_1 \cdot e^{m_1 - m} + l_2 \cdot e^{m_2 - m}\] <p>where \(l_1 = \sum_{i \in I_1} e^{x_i - m_1}\) and \(l_2 = \sum_{i \in I_2} e^{x_i - m_2}\).</p> <p>This incremental computation forms the theoretical foundation for FlashAttention’s tiling strategy, which reduces HBM (High Bandwidth Memory) accesses by computing softmax in SRAM (static RAM) with minimal data movement.</p> <h3 id="implementation-journey-five-gpu-kernels">Implementation Journey: Five GPU Kernels</h3> <p>We implemented five progressively optimized softmax kernels in CUDA to demonstrate different optimization techniques. Let’s examine each approach:</p> <h4 id="kernel-0-the-baseline">Kernel 0: The Baseline</h4> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Simple grid-stride loop</span>
<span class="k">__global__</span> <span class="kt">void</span> <span class="nf">softmax_basic</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">,</span> <span class="kt">int</span> <span class="n">batch_size</span><span class="p">,</span> <span class="kt">int</span> <span class="n">dim</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&gt;=</span> <span class="n">batch_size</span><span class="p">)</span> <span class="k">return</span><span class="p">;</span>
    
    <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">row_input</span> <span class="o">=</span> <span class="n">input</span> <span class="o">+</span> <span class="n">row</span> <span class="o">*</span> <span class="n">dim</span><span class="p">;</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">row_output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">+</span> <span class="n">row</span> <span class="o">*</span> <span class="n">dim</span><span class="p">;</span>
    
    <span class="c1">// Find max</span>
    <span class="kt">float</span> <span class="n">row_max</span> <span class="o">=</span> <span class="o">-</span><span class="n">INFINITY</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">dim</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">row_max</span> <span class="o">=</span> <span class="n">fmaxf</span><span class="p">(</span><span class="n">row_max</span><span class="p">,</span> <span class="n">row_input</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
    <span class="p">}</span>
    
    <span class="c1">// Compute sum of exponentials</span>
    <span class="kt">float</span> <span class="n">sum_exp</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">dim</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">sum_exp</span> <span class="o">+=</span> <span class="n">expf</span><span class="p">(</span><span class="n">row_input</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">row_max</span><span class="p">);</span>
    <span class="p">}</span>
    
    <span class="c1">// Normalize</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">dim</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">row_output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">expf</span><span class="p">(</span><span class="n">row_input</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">row_max</span><span class="p">)</span> <span class="o">/</span> <span class="n">sum_exp</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>This naive implementation suffers from multiple passes over the data and poor memory coalescing.</p> <h4 id="kernel-1-shared-memory-optimization">Kernel 1: Shared Memory Optimization</h4> <p>Kernel 1 introduces shared memory for parallel reduction within a thread block. Each block processes one row, with threads cooperatively computing the maximum and sum through shared memory reductions.</p> <p><strong>Key Insight</strong>: Shared memory is ~100x faster than global memory, making reduction operations significantly faster.</p> <h4 id="kernel-2-warp-level-primitives">Kernel 2: Warp-Level Primitives</h4> <p>Kernel 2 leverages CUDA’s warp shuffle instructions (<code class="language-plaintext highlighter-rouge">__shfl_sync</code>) for efficient warp-level reductions:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__inline__</span> <span class="k">__device__</span> <span class="kt">float</span> <span class="nf">warpReduceMax</span><span class="p">(</span><span class="kt">float</span> <span class="n">val</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">WARP_SIZE</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">offset</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">val</span> <span class="o">=</span> <span class="n">fmaxf</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">__shfl_down_sync</span><span class="p">(</span><span class="mh">0xffffffff</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">offset</span><span class="p">));</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">val</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Advantage</strong>: Warp shuffle instructions bypass shared memory entirely, using register-to-register communication within a warp.</p> <h4 id="kernel-3-2d-block-organization">Kernel 3: 2D Block Organization</h4> <p>Kernel 3 organizes threads in a 2D block (32×4), where each warp processes one row independently. This improves occupancy by allowing multiple warps per block while maintaining efficient warp-level reductions.</p> <h4 id="kernel-4-multi-row-per-warp-with-templates">Kernel 4: Multi-Row Per Warp with Templates</h4> <p>The most sophisticated kernel uses template parameters to process multiple rows per warp:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span><span class="o">&lt;</span><span class="kt">int</span> <span class="n">ROWS_PER_WARP</span><span class="p">,</span> <span class="kt">int</span> <span class="n">COLS_PER_THREAD</span><span class="p">&gt;</span>
<span class="k">__global__</span> <span class="kt">void</span> <span class="nf">softmax_warp_multi_row</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">,</span> 
                                       <span class="kt">int</span> <span class="n">batch_size</span><span class="p">,</span> <span class="kt">int</span> <span class="n">dim</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Each warp processes ROWS_PER_WARP rows</span>
    <span class="c1">// Each thread processes COLS_PER_THREAD elements per row</span>
    <span class="c1">// Uses register caching for maximum performance</span>
<span class="p">}</span>
</code></pre></div></div> <h4 id="performance-comparison">Performance Comparison</h4> <p>We benchmarked all kernels on an NVIDIA RTX 4500 Ada GPU with the following configuration:</p> <ul> <li>Batch size: 32768</li> <li>Dimension: 128</li> <li>Repetitions: 100</li> <li>CUDA version: 12.8</li> </ul> <table> <thead> <tr> <th>Kernel</th> <th>Description</th> <th>Avg Time (ms)</th> <th>Throughput</th> <th>Speedup vs Baseline</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>Baseline</td> <td>0.29562</td> <td>14.2M elements/ms</td> <td>1.0×</td> </tr> <tr> <td>1</td> <td>Shared Memory</td> <td>0.09587</td> <td>43.8M elements/ms</td> <td>3.1×</td> </tr> <tr> <td>2</td> <td>Warp Primitives</td> <td>0.03016</td> <td>139.1M elements/ms</td> <td>9.8×</td> </tr> <tr> <td>3</td> <td>2D Block</td> <td>0.01770</td> <td>236.9M elements/ms</td> <td>16.7×</td> </tr> <tr> <td>4</td> <td>Multi-row per Warp</td> <td>0.01669</td> <td>251.2M elements/ms</td> <td>17.7×</td> </tr> </tbody> </table> <p><br></p> <h4 id="key-observations">Key Observations</h4> <ol> <li> <p><strong>Warp-level optimizations dominate</strong>: Kernels 2-4 show dramatic improvements by leveraging warp shuffle instructions, with Kernel 4 achieving 17.7× speedup over the baseline.</p> </li> <li> <p><strong>Memory hierarchy matters</strong>: Kernel 1’s 3.1× improvement comes primarily from using shared memory for reductions.</p> </li> <li> <p><strong>Algorithm meets architecture</strong>: Kernel 4’s template-based approach demonstrates how algorithm design must consider hardware constraints (warp size, register count).</p> </li> </ol> <h3 id="conclusion">Conclusion</h3> <p>Softmax optimization on GPU demonstrates the beautiful interplay between mathematical theory and hardware architecture, from the basic translation invariance property to sophisticated warp-level implementations. The 17.7× speedup from Kernel 0 to Kernel 4 shows how far we can push performance when we deeply understand both the algorithm and the hardware. These techniques form the foundation for more complex optimizations like FlashAttention.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/flash-attention-1">The details of flash attention - algorithm</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/dllm2">Discrete diffusion model - 2</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/dllm">Discrete diffusion model - 1</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/pseudoinverse">Pseudoinverse Derivation</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/learn-c-5">Learn C++ 5: Inline Function</a> </li> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2026 Guanxiong Luo. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 04, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_code.js" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-L1ZL85W5Y4"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-L1ZL85W5Y4");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>