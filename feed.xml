<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ggluo.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://ggluo.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-12-31T23:42:23+00:00</updated><id>https://ggluo.github.io/feed.xml</id><title type="html">blank</title><subtitle>A computational imaging scientist. </subtitle><entry><title type="html">Optimizing softmax on GPU</title><link href="https://ggluo.github.io/blog/softmax" rel="alternate" type="text/html" title="Optimizing softmax on GPU"/><published>2025-12-29T23:19:00+00:00</published><updated>2025-12-29T23:19:00+00:00</updated><id>https://ggluo.github.io/blog/softmax</id><content type="html" xml:base="https://ggluo.github.io/blog/softmax"><![CDATA[<h3 id="introduction">Introduction</h3> <p>Softmax is a fundamental operation in deep learning, particularly in attention mechanisms of transformer models. While mathematically simple, its efficient implementation on GPU hardware presents interesting challenges and optimization opportunities. In this blog post, we’ll explore the mathematical properties of softmax, implement multiple GPU kernels with increasing sophistication, and analyze their performance characteristics. Complete code is on <a href="https://github.com/ggluo/Minimal_Softmax">GitHub Repository</a>.</p> <h3 id="softmax-translation-invariance">Softmax: Translation Invariance</h3> <p>For a vector \(\mathbf{x} = [x_1, x_2, ..., x_n]\), the softmax function is defined as:</p> \[\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}\] <p>A key property of softmax is its translation invariance:</p> \[\text{softmax}(\mathbf{x} - c) = \text{softmax}(\mathbf{x})\] <p>where \(c\) is any constant. This property is crucial for numerical stability, as we can set #$c = \max(\mathbf{x})#$ to avoid overflow in exponential calculations—this is known as “safe softmax.”</p> <p>This property has a important insight: softmax can be computed incrementally using a reduction approach. If we partition a vector into two intervals \(I_1\) and \(I_2\), we can compute:</p> <p>Let \(m_1 = \max(I_1)\), \(m_2 = \max(I_2)\), and \(m = \max(m_1, m_2)\).</p> <p>Then for the exponential sums:</p> \[l = l_1 \cdot e^{m_1 - m} + l_2 \cdot e^{m_2 - m}\] <p>where \(l_1 = \sum_{i \in I_1} e^{x_i - m_1}\) and \(l_2 = \sum_{i \in I_2} e^{x_i - m_2}\).</p> <p>This incremental computation forms the theoretical foundation for FlashAttention’s tiling strategy, which reduces HBM (High Bandwidth Memory) accesses by computing softmax in SRAM (static RAM) with minimal data movement.</p> <h3 id="implementation-journey-five-gpu-kernels">Implementation Journey: Five GPU Kernels</h3> <p>We implemented five progressively optimized softmax kernels in CUDA to demonstrate different optimization techniques. Let’s examine each approach:</p> <h4 id="kernel-0-the-baseline">Kernel 0: The Baseline</h4> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">// Simple grid-stride loop</span>
<span class="k">__global__</span> <span class="kt">void</span> <span class="nf">softmax_basic</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">,</span> <span class="kt">int</span> <span class="n">batch_size</span><span class="p">,</span> <span class="kt">int</span> <span class="n">dim</span><span class="p">)</span> <span class="p">{</span>
    <span class="kt">int</span> <span class="n">row</span> <span class="o">=</span> <span class="n">blockIdx</span><span class="p">.</span><span class="n">x</span> <span class="o">*</span> <span class="n">blockDim</span><span class="p">.</span><span class="n">x</span> <span class="o">+</span> <span class="n">threadIdx</span><span class="p">.</span><span class="n">x</span><span class="p">;</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">row</span> <span class="o">&gt;=</span> <span class="n">batch_size</span><span class="p">)</span> <span class="k">return</span><span class="p">;</span>
    
    <span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">row_input</span> <span class="o">=</span> <span class="n">input</span> <span class="o">+</span> <span class="n">row</span> <span class="o">*</span> <span class="n">dim</span><span class="p">;</span>
    <span class="kt">float</span><span class="o">*</span> <span class="n">row_output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">+</span> <span class="n">row</span> <span class="o">*</span> <span class="n">dim</span><span class="p">;</span>
    
    <span class="c1">// Find max</span>
    <span class="kt">float</span> <span class="n">row_max</span> <span class="o">=</span> <span class="o">-</span><span class="n">INFINITY</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">dim</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">row_max</span> <span class="o">=</span> <span class="n">fmaxf</span><span class="p">(</span><span class="n">row_max</span><span class="p">,</span> <span class="n">row_input</span><span class="p">[</span><span class="n">i</span><span class="p">]);</span>
    <span class="p">}</span>
    
    <span class="c1">// Compute sum of exponentials</span>
    <span class="kt">float</span> <span class="n">sum_exp</span> <span class="o">=</span> <span class="mf">0.0f</span><span class="p">;</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">dim</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">sum_exp</span> <span class="o">+=</span> <span class="n">expf</span><span class="p">(</span><span class="n">row_input</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">row_max</span><span class="p">);</span>
    <span class="p">}</span>
    
    <span class="c1">// Normalize</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">dim</span><span class="p">;</span> <span class="n">i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">row_output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">expf</span><span class="p">(</span><span class="n">row_input</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">row_max</span><span class="p">)</span> <span class="o">/</span> <span class="n">sum_exp</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div> <p>This naive implementation suffers from multiple passes over the data and poor memory coalescing.</p> <h4 id="kernel-1-shared-memory-optimization">Kernel 1: Shared Memory Optimization</h4> <p>Kernel 1 introduces shared memory for parallel reduction within a thread block. Each block processes one row, with threads cooperatively computing the maximum and sum through shared memory reductions.</p> <p><strong>Key Insight</strong>: Shared memory is ~100x faster than global memory, making reduction operations significantly faster.</p> <h4 id="kernel-2-warp-level-primitives">Kernel 2: Warp-Level Primitives</h4> <p>Kernel 2 leverages CUDA’s warp shuffle instructions (<code class="language-plaintext highlighter-rouge">__shfl_sync</code>) for efficient warp-level reductions:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">__inline__</span> <span class="k">__device__</span> <span class="kt">float</span> <span class="nf">warpReduceMax</span><span class="p">(</span><span class="kt">float</span> <span class="n">val</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">for</span> <span class="p">(</span><span class="kt">int</span> <span class="n">offset</span> <span class="o">=</span> <span class="n">WARP_SIZE</span> <span class="o">/</span> <span class="mi">2</span><span class="p">;</span> <span class="n">offset</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span> <span class="n">offset</span> <span class="o">/=</span> <span class="mi">2</span><span class="p">)</span> <span class="p">{</span>
        <span class="n">val</span> <span class="o">=</span> <span class="n">fmaxf</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> <span class="n">__shfl_down_sync</span><span class="p">(</span><span class="mh">0xffffffff</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">offset</span><span class="p">));</span>
    <span class="p">}</span>
    <span class="k">return</span> <span class="n">val</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>Advantage</strong>: Warp shuffle instructions bypass shared memory entirely, using register-to-register communication within a warp.</p> <h4 id="kernel-3-2d-block-organization">Kernel 3: 2D Block Organization</h4> <p>Kernel 3 organizes threads in a 2D block (32×4), where each warp processes one row independently. This improves occupancy by allowing multiple warps per block while maintaining efficient warp-level reductions.</p> <h4 id="kernel-4-multi-row-per-warp-with-templates">Kernel 4: Multi-Row Per Warp with Templates</h4> <p>The most sophisticated kernel uses template parameters to process multiple rows per warp:</p> <div class="language-cuda highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">template</span><span class="o">&lt;</span><span class="kt">int</span> <span class="n">ROWS_PER_WARP</span><span class="p">,</span> <span class="kt">int</span> <span class="n">COLS_PER_THREAD</span><span class="p">&gt;</span>
<span class="k">__global__</span> <span class="kt">void</span> <span class="nf">softmax_warp_multi_row</span><span class="p">(</span><span class="k">const</span> <span class="kt">float</span><span class="o">*</span> <span class="n">input</span><span class="p">,</span> <span class="kt">float</span><span class="o">*</span> <span class="n">output</span><span class="p">,</span> 
                                       <span class="kt">int</span> <span class="n">batch_size</span><span class="p">,</span> <span class="kt">int</span> <span class="n">dim</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// Each warp processes ROWS_PER_WARP rows</span>
    <span class="c1">// Each thread processes COLS_PER_THREAD elements per row</span>
    <span class="c1">// Uses register caching for maximum performance</span>
<span class="p">}</span>
</code></pre></div></div> <h4 id="performance-comparison">Performance Comparison</h4> <p>We benchmarked all kernels on an NVIDIA RTX 4500 Ada GPU with the following configuration:</p> <ul> <li>Batch size: 32768</li> <li>Dimension: 128</li> <li>Repetitions: 100</li> <li>CUDA version: 12.8</li> </ul> <table> <thead> <tr> <th>Kernel</th> <th>Description</th> <th>Avg Time (ms)</th> <th>Throughput</th> <th>Speedup vs Baseline</th> </tr> </thead> <tbody> <tr> <td>0</td> <td>Baseline</td> <td>0.29562</td> <td>14.2M elements/ms</td> <td>1.0×</td> </tr> <tr> <td>1</td> <td>Shared Memory</td> <td>0.09587</td> <td>43.8M elements/ms</td> <td>3.1×</td> </tr> <tr> <td>2</td> <td>Warp Primitives</td> <td>0.03016</td> <td>139.1M elements/ms</td> <td>9.8×</td> </tr> <tr> <td>3</td> <td>2D Block</td> <td>0.01770</td> <td>236.9M elements/ms</td> <td>16.7×</td> </tr> <tr> <td>4</td> <td>Multi-row per Warp</td> <td>0.01669</td> <td>251.2M elements/ms</td> <td>17.7×</td> </tr> </tbody> </table> <p><br/></p> <h4 id="key-observations">Key Observations</h4> <ol> <li> <p><strong>Warp-level optimizations dominate</strong>: Kernels 2-4 show dramatic improvements by leveraging warp shuffle instructions, with Kernel 4 achieving 17.7× speedup over the baseline.</p> </li> <li> <p><strong>Memory hierarchy matters</strong>: Kernel 1’s 3.1× improvement comes primarily from using shared memory for reductions.</p> </li> <li> <p><strong>Algorithm meets architecture</strong>: Kernel 4’s template-based approach demonstrates how algorithm design must consider hardware constraints (warp size, register count).</p> </li> </ol> <h3 id="conclusion">Conclusion</h3> <p>Softmax optimization on GPU demonstrates the beautiful interplay between mathematical theory and hardware architecture, from the basic translation invariance property to sophisticated warp-level implementations. The 17.7× speedup from Kernel 0 to Kernel 4 shows how far we can push performance when we deeply understand both the algorithm and the hardware. These techniques form the foundation for more complex optimizations like FlashAttention.</p>]]></content><author><name></name></author><category term="cuda"/><category term="self-attention"/><category term="numerical computation"/><category term="coding"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">The details of flash attention - algorithm</title><link href="https://ggluo.github.io/blog/flash-attention-1" rel="alternate" type="text/html" title="The details of flash attention - algorithm"/><published>2025-12-17T17:09:00+00:00</published><updated>2025-12-17T17:09:00+00:00</updated><id>https://ggluo.github.io/blog/flash</id><content type="html" xml:base="https://ggluo.github.io/blog/flash-attention-1"><![CDATA[<p>The compuation of self-attention is a key component in transformer models. It is used to compute the attention weights from the query and key vectors and perform the weighted sum of the value vectors. The attention weights are computed using the dot product of the query and key vectors, and then normalized using the softmax function. This step is computationally expensive in terms of both time and memory as it faces memory bound and compute bound issues.</p> <p>We start with the definition where inputs \(Q, K, V\) typically have shapes \((B, \text{nhead}, N, nd)\). Since batch and head dimensions are independent from attention weights computation, we focus on the sequence length \(N\) and the embedding dimension \(nd\). The illustration below breaks the computation of a single head of self-attention into matrice multiplications. The attention formular is \(\begin{equation} O=\text{Attn}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V~. \end{equation}\) The k-th row of the output matrix is \(\begin{equation} O[k, :]=\sum_{i=1}^N X_s[k, i]\cdot V[i, :]~, \end{equation}\) where \(\begin{align} X_s[k, i]&amp;=\text{softmax}\{i=1:N\vert x_i=X[k,i]=\frac{Q[k, :]K[i, :]^T}{\sqrt{d_k}}\}\\ &amp;=\{i=1:N|\frac{e^{x_i}}{\sum_{j=1}^N e^{x_j}}\}~. \end{align}\)</p> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 1000px)" srcset="/assets/img/blogs/mat_attn-1000.webp"/> <source class="responsive-img-srcset" media="(max-width: 1200px)" srcset="/assets/img/blogs/mat_attn-1200.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/blogs/mat_attn-1400.webp"/> <img src="/assets/img/blogs/mat_attn.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" title="computation of self-attention" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption_post" style="margin-bottom: 1.15rem"> </div> </div> <p>We cannot simply exponentiate the dot products because \(e^{x_i}\) can easily overflow floating-point limits. To fix this, we use safe softmax. This requires finding the maximum value in the row (\(m_N\)) and subtracting it from every element before exponentiation. Then, this computation involves three distinct steps (loops) over the data:</p> <ol> <li> <p>Find the Global Max (\(m_N\)): Iterate through the row to find \(m_N = \max(x_i)\).</p> </li> <li> <p>Compute the Global Sum (\(d_T\)): Iterate again to compute the normalization constant \(d_T = \sum_{i=1}^{N} e^{x_i - m_N}\).</p> </li> <li> <p>Compute the Output: Iterate a third time to calculate the weighted sum \(O[k, :] = \sum_{i=1}^{N} \frac{e^{x_i - m_N}}{d_T} \cdot V[i, :]\).</p> </li> </ol> <p>This approach forces us to separate the calculation into multiple passes. We cannot compute the output until we have finished scanning the entire row to find \(m_N\) and \(d_T\). This requires repeated reading and writing of the \(N \times N\) matrix makes standard attention memory-bounded.</p> <p>To optimize this, we must ask: Can we merge these main blocks?.</p> <p>We need a way to calculate the softmax incrementally (online) as we read the data, rather than waiting for the global statistics. This is achieved by maintaining running statistics that update dynamically. Instead of calculating the final max and sum upfront, we define running variables for the \(i\)-th step: 1) the running maximum: \(m_i\); 2) the running sum of exponentials: \(d'_i=\sum_{j=1}^{i} e^{x_j - m_i}\).</p> <p>Then we have two main steps. In loop 1: \(i=1 \rightarrow N\) when moving from step \(i-1\) to \(i\), we introduce a new value \(x_i\). If \(x_i &gt; m_{i-1}\), our maximum changes. We must rescale our previous accumulations to account for this change.</p> <ol> <li> <p>Updating the Max: \(m_i = \max(m_{i-1}, x_i)\)</p> </li> <li> <p>Updating the Sum (\(d'_i\)) by rescaling the previous sum (\(d'_{i-1}\)) by the exponential difference between the old and new max: \(d'_i = d'_{i-1} \cdot e^{m_{i-1} - m_i} + e^{x_i - m_i}\)</p> </li> </ol> <p>Upon reaching the N-th step, we have \(d'_T=d_T\). Then we start loop 2: \(i=1 \rightarrow N\), \(O[k, :] = \sum_{i=1}^{N} \frac{e^{x_i - m_N}}{d'_T} \cdot V[i, :]\)</p> <p>Can we merge these two loops? Yes, but the second loop is dependent on \(m_N\) and \(d'_T\). Then, we apply the same rescaling logic to the partial output accumulator by defining \(O'_i = \sum_{j=1}^i \frac{e^{x_j - m_i}}{d'_i} \cdot V[j, :]\). As a result, the computation of the output matrix \(O[k, :]\) can be recast into a geometric progression:</p> \[\begin{align} O'_i &amp;= \sum_{j=1}^i \frac{e^{x_j - m_i}}{d'_i} \cdot V[j, :] \\ &amp; = \sum_{j=1}^{i-1} \frac{e^{x_j - m_i}}{d'_i} \cdot V[j, :] + \frac{e^{x_i - m_i}}{d'_i} \cdot V[j, :] \\ &amp; = \sum_{j=1}^{i-1} \frac{e^{x_j - m_{i-1}}}{d'_{i-1}} \cdot \left( \frac{d'_{i-1}}{e^{x_j - m_{i-1}}} \cdot \frac{e^{x_j - m_i}}{d'_i} \right) \cdot V[j, :] + \frac{e^{x_i - m_i}}{d'_i} \cdot V[i, :] \\ &amp; = O'_{i-1} \cdot \left( \frac{d'_{i-1}}{d'_i} \cdot e^{m_{i-1} - m_i} \right) + \frac{e^{x_i - m_i}}{d'_i} \cdot V[i, :] \end{align}\] <p>Upon reaching the N-th step with these recursive formulas, we have \(O'_T[k, :] = O[k, :]\). By doing this, we can compute the output in a single pass without needing the computation of \(X\) matrix, and write/read it repeatedly to get global max and sum upfront. The next blog post will focus on how to implement this in an efficient way using CUDA.</p>]]></content><author><name></name></author><category term="generative models"/><category term="self-attention"/><category term="coding"/><summary type="html"><![CDATA[The compuation of self-attention is a key component in transformer models. It is used to compute the attention weights from the query and key vectors and perform the weighted sum of the value vectors. The attention weights are computed using the dot product of the query and key vectors, and then normalized using the softmax function. This step is computationally expensive in terms of both time and memory as it faces memory bound and compute bound issues.]]></summary></entry><entry><title type="html">Discrete diffusion model - 2</title><link href="https://ggluo.github.io/blog/dllm2" rel="alternate" type="text/html" title="Discrete diffusion model - 2"/><published>2025-11-08T20:38:00+00:00</published><updated>2025-11-08T20:38:00+00:00</updated><id>https://ggluo.github.io/blog/dllm2</id><content type="html" xml:base="https://ggluo.github.io/blog/dllm2"><![CDATA[<p>In this post, I’ll try to explain the core idea behind discrete diffusion models. Let’s define our setting:</p> <ul> <li><strong>Data (\(x_0\)):</strong> A sequence of tokens \(x_0 = (x_0^{(1)}, x_0^{(2)}, \dots, x_0^{(L)})\).</li> <li><strong>Vocabulary (\(V\)):</strong> Each token comes from a discrete vocabulary of size \(K\). For text, this is \(V = \{\text{"a"}, \text{"aardvark"}, \dots, \text{"zygote"}, \text{"[MASK]"}\}\).</li> <li><strong>Representation:</strong> We can represent a single token \(x^{(i)}\) as a <strong>one-hot vector</strong> of size \(K\).</li> </ul> <p>The core idea is to replace the <em>addition of Gaussian noise</em> with <em>multiplication by a transition matrix</em>.</p> <hr/> <p>The forward process \(q(x_t \vert x_{t-1})\) defines the probability of transitioning from a token at step \(t-1\) to a token at step \(t\). This is defined by a \(K \times K\) stochastic matrix \(Q_t\). The element \([Q_t]_{ij}\) represents the probability of a token \(i\) turning into a token \(j\) in one step.</p> <p>\begin{equation} q(x_t^{(i)} = j \vert x_{t-1}^{(i)} = i) = [Q_t]_{ij} \end{equation}</p> <p>If we use one-hot vectors for \(x_{t-1}\) (where it’s a row vector), this transition is a simple matrix multiplication:</p> <p>\begin{equation} q(x_t \vert x_{t-1}) = \text{Cat}(x_t ; p = x_{t-1} Q_t) \end{equation} This means the probability distribution for \(x_t\) is a Categorical distribution, with probabilities given by the vector \(x_{t-1} Q_t\). This matrix \(Q_t\) is <strong>not learned</strong>; it is a fixed schedule, just like the \(\beta_t\) schedule in continuous models. The key is in <em>how</em> we design \(Q_t\). Here are the two most common designs for text:</p> <h5 id="design-1-uniform-token-switching-diffusion">Design 1: Uniform (“Token-Switching”) Diffusion</h5> <p>This matrix says: at step \(t\), with probability \(1-\beta_t\) keep the token the same, and with probability \(\beta_t\), switch it to <em>any</em> token in the vocabulary (including itself) with uniform probability \(1/K\).</p> <p>Let \(\beta_t\) be the “noise” schedule. The matrix \(Q_t\) is: \(Q_t = (1 - \beta_t) \cdot I + \beta_t \cdot \frac{1}{K} \mathbb{1}\mathbb{1}^T\)</p> <ul> <li>\(I\) is the \(K \times K\) identity matrix.</li> <li>\(\mathbb{1}\mathbb{1}^T\) is a \(K \times K\) matrix of all ones.</li> <li>\([Q_t]_{ij} = (1 - \beta_t)\) if \(i=j\), and \(\beta_t/K\) if \(i \neq j\). (Note: the diagonal is actually \((1-\beta_t) + \beta_t/K\))</li> </ul> <h5 id="design-2-absorbing-masking-diffusion">Design 2: Absorbing (“Masking”) Diffusion</h5> <p>This is the most common for text, as it’s analogous to BERT. We add a special <code class="language-plaintext highlighter-rouge">[MASK]</code> token to the vocabulary (let’s say it’s at index \(m\)).</p> <p>This matrix says: at step \(t\), with probability \(1-\beta_t\) keep the token the same, and with probability \(\beta_t\), <strong>replace it with the <code class="language-plaintext highlighter-rouge">[MASK]</code> token</strong>. Once a token becomes <code class="language-plaintext highlighter-rouge">[MASK]</code>, it can <em>never</em> change back.</p> <p>The matrix \(Q_t\) looks like this:</p> \[\begin{equation} [Q_t]_{ij} = \left\{ \begin{array}{ll} 1 &amp; \text{if } i = j = m \text{ ([MASK] stays [MASK])} \\ 1 - \beta_t &amp; \text{if } i = j \neq m \text{ (Token stays itself)} \\ \beta_t &amp; \text{if } j = m,\, i \neq m \text{ (Token becomes [MASK])} \\ 0 &amp; \text{otherwise ([MASK] never becomes a token)} \end{array} \right. \end{equation}\] <p>This is an “absorbing state,” which is why it’s called absorbing diffusion.</p> <p>Just like in continuous models, we want to jump from \(x_0\) to \(x_t\) in one step. Thanks to the Markov property, this is just matrix multiplication:</p> <p>\begin{equation} \bar{Q}_t = Q_1 Q_2 \dots Q_t \end{equation} Then, the probability of \(x_t\) given \(x_0\) is:</p> <p>\begin{equation} q(x_t \vert x_0) = \text{Cat}(x_t ; p = x_0 \bar{Q}_t) \end{equation} This \(\bar{Q}_t\) matrix is pre-calculated and fixed. The element \([\bar{Q}_t]_{ij}\) gives the total probability that an initial token \(i\) will have become token \(j\) after \(t\) steps of noising.</p> <p>This is the “denoising” step where the neural network comes in. We need to learn a model \(p_\theta\) that approximates the “ground truth” posterior \(q(x_{t-1} \vert x_t, x_0)\).</p> <p>Using Bayes’ theorem, we can derive this ground truth posterior:</p> <p>\begin{equation} q(x_{t-1} \vert x_t, x_0) \propto q(x_t \vert x_{t-1}, x_0) \cdot q(x_{t-1} \vert x_0) \end{equation} Because the forward process is Markovian, \(q(x_t \vert x_{t-1}, x_0) = q(x_t \vert x_{t-1})\).</p> <p>\begin{equation} q(x_{t-1} \vert x_t, x_0) \propto \underbrace{q(x_t \vert x_{t-1})}_{\text{from } Q_t} \cdot \underbrace{q(x_{t-1} \vert x_0)}_{\text{from} \bar{Q}_{t-1}} \end{equation}</p> <p>This gives us a (very messy) but <strong>fully computable</strong> categorical distribution for \(x_{t-1}\). In one-hot vector notation (with \(\odot\) as element-wise multiplication):</p> <p>\begin{equation} p = \underbrace{x_t (Q_t)^T}_{\text{Prob. of } x_t \text{ from } x_{t-1}} \odot \underbrace{x_0 \bar{Q}_{t-1}}_{\text{Prob. of } x_{t-1} \text{ from } x_0} \end{equation}</p> <p>\begin{equation} q(x_{t-1} \vert x_t, x_0) = \text{Cat}\left(x_{t-1} ; p’ = \frac{p}{\sum p}\right) \end{equation}</p> <p>This formula tells us, “Given the corrupted \(x_t\) and the original \(x_0\), what is the exact probability distribution for what \(x_{t-1}\) was?”</p> <p>The model \(p_\theta(x_{t-1} \vert x_t)\) is trained to match this ground truth distribution.</p> <ol> <li><strong>Model \(\theta\):</strong> This is typically a Transformer. It takes \(x_t\) and the timestep \(t\) as input.</li> <li><strong>Prediction:</strong> The Transformer’s job is to predict \(\hat{x}_0\) (the “clean” text). <ul> <li> \[\hat{p}_0 = \text{softmax}(\text{Transformer}(x_t, t))\] </li> <li> \[\hat{x}_0 = \text{one-hot}(\text{argmax}(\hat{p}_0))\] </li> </ul> </li> <li><strong>Loss:</strong> The model \(p_\theta\) is formed by plugging this \(\hat{x}_0\) prediction into the Bayes formula from step 3.</li> </ol> <p>\begin{equation} p_\theta(x_{t-1} \vert x_t) \approx q(x_{t-1} \vert x_t, \hat{x}_0) \end{equation}</p> <p>The training loss is then a <strong>Cross-Entropy</strong> loss (or KL divergence) that forces this \(p_\theta\) distribution to match the true \(q(x_{t-1} \vert x_t, x_0)\) distribution.</p> <p>In simple terms: the Transformer (like BERT) is trained to look at a corrupted sentence (e.g., “The <code class="language-plaintext highlighter-rouge">[MASK]</code> brown fox”) and predict the original (“The quick brown fox”). This prediction is then used to calculate the “denoising” probability for the previous timestep.</p>]]></content><author><name></name></author><category term="generative models"/><category term="diffusion models"/><summary type="html"><![CDATA[In this post, I’ll try to explain the core idea behind discrete diffusion models. Let’s define our setting:]]></summary></entry><entry><title type="html">Discrete diffusion model - 1</title><link href="https://ggluo.github.io/blog/dllm" rel="alternate" type="text/html" title="Discrete diffusion model - 1"/><published>2025-11-08T20:04:00+00:00</published><updated>2025-11-08T20:04:00+00:00</updated><id>https://ggluo.github.io/blog/dllm</id><content type="html" xml:base="https://ggluo.github.io/blog/dllm"><![CDATA[<p>Discrete diffusion model is getting viral. People from image generation field are wondering if we could use accelerated sampling to generate discrete sequences. However, this is hard to achieve with simple ideas, e.g., DDIM, that we have seen in continuous diffusion models used in image generation. The fundamental challenge stems from the difference between <strong>continuous</strong> and <strong>discrete</strong> state spaces.</p> <p>The mathematical “trick” of DDIM relies entirely on the data living in a continuous vector space (like \(\mathbb{R}^n\), where images live).</p> <p>Let’s look at the deterministic DDIM equation again: \(\begin{equation} x_{t-1} = \underbrace{\sqrt{\bar{\alpha}_{t-1}} \hat{x}_0(x_t, t)}_{\text{Component 1}} + \underbrace{\sqrt{1 - \bar{\alpha}_{t-1}} \cdot \epsilon_\theta(x_t, t)}_{\text{Component 2}} \end{equation}\)</p> <p>This equation is a <strong>linear interpolation</strong> in a vector space. It works because:</p> <ol> <li><strong>Scaling:</strong> You can scale a vector (an image) by a scalar like \(\sqrt{\bar{\alpha}_{t-1}}\).</li> <li><strong>Addition:</strong> You can add two vectors (the “clean” component and the “noise” component) together.</li> </ol> <p><strong>In a discrete diffusion model (like for text), these operations are meaningless.</strong></p> <ul> <li><strong>The Data:</strong> Your data \(x_t\) is not a vector of real numbers. It’s a sequence of integers (token IDs) from a finite vocabulary \(V = \{1, 2, ..., \vert V \vert\}\).</li> <li><strong>The “Noise”:</strong> The forward process isn’t adding Gaussian noise. It’s applying a <em>probabilistic transition matrix</em> \(Q_t\). This usually means “randomly replace a token with <code class="language-plaintext highlighter-rouge">[MASK]</code>” or “randomly replace a token with another token.”</li> <li><strong>The Problem:</strong> What is \(\sqrt{0.5} \times \text{"hello"}\)? What is \(\text{"hello"} + \text{"world"}\)? These operations are undefined. You cannot perform linear interpolation on token IDs.</li> </ul> <p>Because the math of DDPM/DDIM doesn’t apply, discrete diffusion models (like D3PM or Mask-GIT) use a completely different mathematical framework.</p> <ol> <li><strong>Forward Process \(q(x_t \vert x_{t-1})\):</strong> <ul> <li>This is defined by a <strong>transition matrix</strong> \(Q_t\).</li> <li>For example, \(Q_t(j \vert i)\) might be the probability of token \(i\) at \(t-1\) becoming token \(j\) at time \(t\). (e.g., 90% chance of staying the same, 5% chance of becoming <code class="language-plaintext highlighter-rouge">[MASK]</code>, 5% chance of becoming a random token).</li> </ul> </li> <li><strong>Reverse Process \(p_\theta(x_{t-1} \vert x_t)\):</strong> <ul> <li>The model \(\theta\) is trained to <em>invert</em> this transition.</li> <li>It takes the corrupted text \(x_t\) (e.g., “The <code class="language-plaintext highlighter-rouge">[MASK]</code> brown fox”) and predicts the probability distribution for the <em>previous</em> state \(x_{t-1}\) (e.g., “The <code class="language-plaintext highlighter-rouge">[MASK]</code> brown fox”).</li> <li>Crucially, the model outputs a <strong>categorical distribution</strong> over the entire vocabulary for each token.</li> <li> \[p_\theta(x_{t-1} \vert x_t) = \text{softmax}(\dots)\] </li> </ul> </li> <li><strong>Sampling (The “Denoising” Step):</strong> <ul> <li>To get \(x_{t-1}\) from \(x_t\), you must <strong>sample</strong> from this predicted categorical distribution.</li> <li> \[x_{t-1} \sim p_\theta(\cdot \vert x_t)\] </li> <li>This step is <em>inherently stochastic</em>. You are drawing a token from a probability distribution.</li> </ul> </li> </ol> <h4 id="so-what-is-the-discrete-deterministic-equivalent">So, what is the discrete “deterministic” equivalent?</h4> <p>You can’t have the DDIM-style interpolation, but you <em>can</em> make the discrete sampling step deterministic. The closest equivalent to a “deterministic” step in a discrete model is not DDIM, but <strong>greedy decoding (i.e., <code class="language-plaintext highlighter-rouge">argmax</code>)</strong>. Instead of <em>sampling</em> from the predicted distribution \(p_\theta(\cdot \vert x_t)\), you just choose the single most likely token at each position:</p> <p>\begin{equation} x_{t-1} = \text{argmax}(p_\theta(\cdot \vert x_t)) \end{equation}</p> <p>This is deterministic, but it is not “DDIM” and doesn’t share its mathematical properties. It’s simply greedy sampling, which often leads to lower-quality and less diverse results compared to stochastic sampling. The challenge of creating a “fast sampling” method for discrete models that <em>also</em> maintains high quality (the way DDIM does for continuous models) is an active area of research. These methods are typically called “ancestral sampling” modifications, not “implicit” models.</p>]]></content><author><name></name></author><category term="generative models"/><category term="diffusion models"/><summary type="html"><![CDATA[Discrete diffusion model is getting viral. People from image generation field are wondering if we could use accelerated sampling to generate discrete sequences. However, this is hard to achieve with simple ideas, e.g., DDIM, that we have seen in continuous diffusion models used in image generation. The fundamental challenge stems from the difference between continuous and discrete state spaces.]]></summary></entry><entry><title type="html">Pseudoinverse Derivation</title><link href="https://ggluo.github.io/blog/pseudoinverse" rel="alternate" type="text/html" title="Pseudoinverse Derivation"/><published>2025-11-06T13:53:00+00:00</published><updated>2025-11-06T13:53:00+00:00</updated><id>https://ggluo.github.io/blog/pseudoinverse</id><content type="html" xml:base="https://ggluo.github.io/blog/pseudoinverse"><![CDATA[<p>The concept of a pseudoinverse arises because we often encounter linear systems \(Ax = b\) that don’t have a unique, exact solution. The least-squares framework provides a way to find the “best” possible solution. This derivation is typically split into two main cases, both of which fall under the umbrella of “least-squares.”</p> <hr/> <h4 id="1-the-overdetermined-case-tall-matrix-m--n">1. The overdetermined case (tall matrix, \(m &gt; n\))</h4> <p>This is the classic least-squares problem. We have more equations (\(m\)) than unknowns (\(n\)), and the system is likely inconsistent (meaning no \(x\) perfectly satisfies \(Ax = b\)).</p> <p><strong>The Goal:</strong> Find the vector \(x\) that minimizes the squared error, or the L2-norm of the residual vector \(r = Ax - b\). \begin{equation} x^* = \arg \min_{x} |Ax - b|^2 \end{equation}</p> <p>Let’s define the squared error \(E(x)\) as a function of \(x\). Using the dot product, the squared norm is \(r^T r\): \begin{equation} E(x) = (Ax - b)^T (Ax - b) \end{equation} We expand the terms:</p> \[\begin{aligned} E(x)&amp;= (x^T A^T - b^T) (Ax - b) \\ E(x)&amp;= x^T A^T A x - x^T A^T b - b^T A x + b^T b \end{aligned}\] <p>Notice that \(x^T A^T b\) and \(b^T A x\) are both scalars. In fact, they are transposes of each other, and since the transpose of a scalar is just the scalar itself, they are equal: \((b^T A x)^T = x^T A^T b\). So, we can combine them: \begin{align} E(x) = x^T A^T A x - 2 x^T A^T b + b^T b \end{align}</p> <p>To find the \(x\) that minimizes this error, we take the gradient of \(E(x)\) with respect to \(x\) and set it to zero. Using the matrix calculus rules:</p> <ul> <li>\(\nabla_x (x^T B x) = (B + B^T) x\) (which is \(2Bx\) if \(B\) is symmetric)</li> <li>\(\nabla_x (c^T x) = c\) (or \(\nabla_x (x^T c) = c\))</li> </ul> <p>The matrix \(A^T A\) is symmetric. Applying these rules, we get:</p> <p>\begin{equation} \nabla_x E(x) = 2(A^T A) x - 2(A^T b) + 0 \end{equation}</p> <p>Set the gradient to zero to find the minimum: \(\begin{align} 2(A^T A) x - 2(A^T b) =&amp; 0 \\ (A^T A) x =&amp; A^T b \end{align}\)</p> <p>This final equation, \(A^T A x = A^T b\), is known as the <strong>normal equation</strong>. To solve for \(x\), we can multiply by the inverse of \((A^T A)\) and get \(x = (A^T A)^{-1} A^T b\). This step requires a critical assumption: <strong>\((A^T A)\) must be invertible</strong>. This is true if and only if \(A\) has <strong>linearly independent columns</strong> (i.e., \(A\) has full column rank).</p> <p>We are looking for a matrix \(A^+\) (the pseudoinverse) such that \(x = A^+ b\). By direct comparison, we find the <strong>left pseudoinverse</strong>:</p> <p>\begin{equation} A^+ = (A^T A)^{-1} A^T \end{equation}</p> <p>It’s called the “left” inverse because if you multiply \(A\) on the left by \(A^+\), you get the identity matrix: \(A^+ A = \left[ (A^T A)^{-1} A^T \right] A = (A^T A)^{-1} (A^T A) = I\).</p> <hr/> <h4 id="2-the-underdetermined-case-wide-matrix-n--m">2. The underdetermined case (wide matrix, \(n &gt; m\))</h4> <p>This is a different kind of “least-squares” problem. We have fewer equations (\(m\)) than unknowns (\(n\)), so the system $Ax = b$ has infinitely many solutions.</p> <p><strong>The Goal:</strong> Of all the possible solutions, find the <em>unique</em> solution $x$ that has the <strong>minimum norm</strong> (\(\min \|x\|^2\)). This is the “least-squares” solution in the sense that it’s the “smallest” solution vector. We want to solve the constrained optimization problem:</p> <p>\begin{equation} \min |x|^2 \quad \text{subject to} \quad Ax = b \end{equation}</p> <p>We can solve this using the method of Lagrange multipliers. The Lagrangian function \(L\) is:</p> <p>\begin{equation} L(x, \lambda) = x^T x + \lambda^T (Ax - b) \end{equation}</p> <p>Here, \(x^T x\) is \(\|x\|^2\) and \(\lambda\) is a vector of Lagrange multipliers. We find the minimum by setting the gradients with respect to both \(x\) and \(\lambda\) to zero.</p> <p><strong>Gradient w.r.t. \(x\):</strong></p> \[\begin{aligned} \nabla_x L &amp;= 2x + A^T \lambda = 0\\ x &amp;= -\frac{1}{2} A^T \lambda \end{aligned}\] <p><strong>Gradient w.r.t. \(\lambda\):</strong></p> \[\begin{aligned} \nabla_\lambda L &amp;= Ax - b = 0\\ Ax &amp;= b \end{aligned}\] <p>Now we combine these two results. Substitute the expression for \(x\) into the constraint equation:</p> \[\begin{aligned} A \left( -\frac{1}{2} A^T \lambda \right) &amp;= b\\ -\frac{1}{2} (A A^T) \lambda &amp;= b \end{aligned}\] <p>Now, solve for the multiplier vector: \(\lambda = -2 (A A^T)^{-1} b\)</p> <p>This step requires the assumption that <strong>\((A A^T)\) is invertible</strong>, which is true if and only if $A$ has <strong>linearly independent rows</strong> (i.e., \(A\) has full row rank).</p> <hr/> <p>The full <strong>Moore-Penrose pseudoinverse</strong> (often derived using Singular Value Decomposition, or SVD) generalizes both of these cases. It also works for matrices that are <em>rank-deficient</em> (i.e., neither full column nor full row rank), which is when \((A^T A)\) or \((A A^T)\) would be singular (non-invertible).</p> <p>However, the “least-squares” perspective naturally gives rise to these two fundamental forms:</p> <table> <thead> <tr> <th style="text-align: left">Case</th> <th style="text-align: left">Matrix Shape</th> <th style="text-align: left">Problem</th> <th style="text-align: left">Solution \(x = A^+ b\)</th> <th style="text-align: left">Pseudoinverse \(A^+\)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left"><strong>Overdetermined</strong></td> <td style="text-align: left">Tall (\(m &gt; n\)), full column rank</td> <td style="text-align: left">\(\min |Ax - b|^2\)</td> <td style="text-align: left">Least-Squares Solution</td> <td style="text-align: left">\(A^+ = (A^T A)^{-1} A^T\)</td> </tr> <tr> <td style="text-align: left"><strong>Underdetermined</strong></td> <td style="text-align: left">Wide (\(n &gt; m\)), full row rank</td> <td style="text-align: left">\(\min |x|^2\) s.t. \(Ax=b\)</td> <td style="text-align: left">Minimum Norm Solution</td> <td style="text-align: left">\(A^+ = A^T (A A^T)^{-1}\)</td> </tr> </tbody> </table> <hr/> <h4 id="3-pseudoinverse-using-singular-value-decomposition">3. Pseudoinverse using singular value decomposition</h4> <p>Here is the derivation of the pseudoinverse using the Singular Value Decomposition (SVD). This is the most general and powerful approach, as it works for <strong>all</strong> matrices, including those that are rank-deficient (where the previous least-squares derivations would fail).</p> <p>First, any \(m \times n\) matrix \(A\) can be decomposed using SVD as:</p> <p>\begin{equation} A = U \Sigma V^T \end{equation}</p> <p>Where:</p> <ul> <li><strong>\(U\)</strong>: An \(m \times m\) orthogonal matrix (\(U^T U = I_m\)). Its columns are the <em>left singular vectors</em>.</li> <li><strong>\(V\)</strong>: An \(n \times n\) orthogonal matrix (\(V^T V = I_n\)). Its columns are the <em>right singular vectors</em>.</li> <li><strong>\(\Sigma\)</strong>: An \(m \times n\) diagonal matrix containing the singular values \(\sigma_1, \sigma_2, ...\) in decreasing order. These values are non-negative.</li> </ul> <p>If \(A\) has rank \(r\), then \(r\) of the singular values are positive, and the rest are zero.</p> <p>The key to the SVD approach is defining the pseudoinverse of the simple diagonal matrix \(\Sigma\).</p> <p>Let’s say \(\Sigma\) (an \(m \times n\) matrix) looks like this:</p> \[\Sigma = \begin{bmatrix} \sigma_1 &amp; 0 &amp; \dots &amp; 0 \\ 0 &amp; \sigma_2 &amp; \dots &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \dots &amp; \sigma_n \\ \vdots &amp; \vdots &amp; \dots &amp; \vdots \\ 0 &amp; 0 &amp; \dots &amp; 0 \end{bmatrix}\] <p>To find its pseudoinverse \(\Sigma^+\), we do two things:</p> <ol> <li><strong>Transpose</strong> its dimensions to get an \(n \times m\) matrix.</li> <li><strong>Reciprocate</strong> all the <strong>non-zero</strong> singular values, leaving the zeros as zeros.</li> </ol> <p>If \(\Sigma_{ii} = \sigma_i &gt; 0\), then \((\Sigma^+)_{ii} = 1/\sigma_i\). If \(\Sigma_{ii} = 0\), then \((\Sigma^+)_{ii} = 0\).</p> <p><strong>Example:</strong></p> <p>If</p> \[\Sigma = \begin{bmatrix} \sigma_1 &amp; 0 &amp; 0 \\ 0 &amp; \sigma_2 &amp; 0 \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{bmatrix}\] <p>then</p> \[\Sigma^+ = \begin{bmatrix} 1/\sigma_1 &amp; 0 &amp; 0 &amp; 0 \\ 0 &amp; 1/\sigma_2 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0 &amp; 0 \end{bmatrix}\] <p>With \(\Sigma^+\) defined, the pseudoinverse of \(A\) (called the <strong>Moore-Penrose Pseudoinverse</strong>) is simply:</p> <p>\begin{equation} A^+ = V \Sigma^+ U^T \end{equation}</p> <p>This definition elegantly handles all cases: overdetermined, underdetermined, and rank-deficient. Now, let’s show that this SVD definition is consistent with the two cases we derived earlier.</p> <h5 id="case-1-overdetermined-tall-matrix-m--n-full-column-rank-rn">Case 1: overdetermined (tall matrix, \(m &gt; n\), full column rank \(r=n\))</h5> <ul> <li>Our previous formula was \(A^+ = (A^T A)^{-1} A^T\).</li> <li>Let’s plug the SVD into this formula and see if we get \(V \Sigma^+ U^T\).</li> </ul> <ol> <li> <p><strong>\(A^T A\)</strong>: \(A^T A = (U \Sigma V^T)^T (U \Sigma V^T) = (V \Sigma^T U^T) (U \Sigma V^T)\) Since \(U^T U = I\), this simplifies to: \(A^T A = V (\Sigma^T \Sigma) V^T\)</p> </li> <li> <p><strong>\((A^T A)^{-1}\)</strong>: \((A^T A)^{-1} = (V (\Sigma^T \Sigma) V^T)^{-1}\) Using the rule \((XYZ)^{-1} = Z^{-1} Y^{-1} X^{-1}\) and knowing \(V^{-1} = V^T\): \((A^T A)^{-1} = (V^T)^{-1} (\Sigma^T \Sigma)^{-1} V^{-1} = V (\Sigma^T \Sigma)^{-1} V^T\)</p> </li> <li> <p><strong>\(A^+ = (A^T A)^{-1} A^T\)</strong>: \(A^+ = \left[ V (\Sigma^T \Sigma)^{-1} V^T \right] (U \Sigma V^T)^T\) \(A^+ = \left[ V (\Sigma^T \Sigma)^{-1} V^T \right] (V \Sigma^T U^T)\) Since \(V^T V = I\), this simplifies to: \(A^+ = V (\Sigma^T \Sigma)^{-1} \Sigma^T U^T\)</p> </li> <li>analyze the \(\Sigma\) part: <ul> <li>\(\Sigma\) is \(m \times n\). Let’s say \(\Sigma = \begin{bmatrix} D_n \\ 0 \end{bmatrix}\), where \(D_n\) is an \(n \times n\) diagonal matrix of \(\sigma_i\).</li> <li>\(\Sigma^T = \begin{bmatrix} D_n &amp; 0 \end{bmatrix}\).</li> <li>\(\Sigma^T \Sigma = \begin{bmatrix} D_n &amp; 0 \end{bmatrix} \begin{bmatrix} D_n \\ 0 \end{bmatrix} = D_n^2\). (This is an \(n \times n\) invertible matrix).</li> <li>\((\Sigma^T \Sigma)^{-1} = (D_n^2)^{-1} = D_n^{-2}\).</li> <li>Now, \((\Sigma^T \Sigma)^{-1} \Sigma^T = D_n^{-2} \begin{bmatrix} D_n &amp; 0 \end{bmatrix} = \begin{bmatrix} D_n^{-1} &amp; 0 \end{bmatrix}\).</li> <li>This resulting matrix, \(\begin{bmatrix} D_n^{-1} &amp; 0 \end{bmatrix}\), is <strong>exactly the SVD definition of \(\Sigma^+\)</strong>.</li> </ul> </li> <li>Conclusion: \(A^+ = V (\Sigma^+) U^T\). The formulas match.</li> </ol> <h5 id="case-2-underdetermined-wide-matrix-n--m-full-row-rank-rm">Case 2: underdetermined (wide matrix, \(n &gt; m\), full row rank \(r=m\))</h5> <ul> <li>Our previous formula was \(A^+ = A^T (A A^T)^{-1}\).</li> <li> <p>A similar substitution (which I’ll skip the full algebra for) shows: \(A^+ = (V \Sigma^T U^T) \left[ (U \Sigma V^T)(V \Sigma^T U^T) \right]^{-1}\) \(A^+ = (V \Sigma^T U^T) \left[ U (\Sigma \Sigma^T) U^T \right]^{-1}\) \(A^+ = (V \Sigma^T U^T) \left[ U (\Sigma \Sigma^T)^{-1} U^T \right]\) \(A^+ = V \Sigma^T (\Sigma \Sigma^T)^{-1} U^T\)</p> </li> <li>Again, if you analyze the \(\Sigma\) part (\(\Sigma^T (\Sigma \Sigma^T)^{-1}\)), you’ll find it simplifies to \(\Sigma^+\).</li> <li><strong>Conclusion</strong>: \(A^+ = V \Sigma^+ U^T\). The formulas match again.</li> </ul> <h5 id="why-the-svd-method-is-better">Why the SVD Method is Better</h5> <p>The SVD derivation \(A^+ = V \Sigma^+ U^T\) is superior because it <strong>never fails</strong>.</p> <ul> <li>In the rank-deficient case, \(r &lt; \min(m, n)\).</li> <li>This means both \((A^T A)\) and \((A A^T)\) will be singular (non-invertible) because their \(\Sigma\) components (\(D_r^2\)) will contain zeros. Our first derivation using \((...)^{-1}\) would fail.</li> <li>However, the SVD definition of \(\Sigma^+\) <strong>gracefully handles this</strong>. It only reciprocates the \(\sigma_i\) that are <em>non-zero</em> and leaves the zeros as zeros.</li> </ul> <p>This \(A^+ = V \Sigma^+ U^T\) always gives the unique <strong>minimum-norm least-squares solution</strong>:</p> <ol> <li><strong>Least-Squares</strong>: It finds the \(x\) (or \(x\)’s) that minimizes the error \(\|Ax - b\|^2\).</li> <li><strong>Minimum-Norm</strong>: If there are multiple solutions (as in the underdetermined or rank-deficient case), it picks the <em>one and only one</em> solution \(x\) that also has the smallest length \(\|x\|^2\).</li> </ol>]]></content><author><name></name></author><category term="linear algebra"/><category term="coding"/><summary type="html"><![CDATA[The concept of a pseudoinverse arises because we often encounter linear systems \(Ax = b\) that don’t have a unique, exact solution. The least-squares framework provides a way to find the “best” possible solution. This derivation is typically split into two main cases, both of which fall under the umbrella of “least-squares.”]]></summary></entry><entry><title type="html">Learn C++ 5: Inline Function</title><link href="https://ggluo.github.io/blog/learn-c-5" rel="alternate" type="text/html" title="Learn C++ 5: Inline Function"/><published>2023-03-11T07:28:00+00:00</published><updated>2023-03-11T07:28:00+00:00</updated><id>https://ggluo.github.io/blog/learn-c-5</id><content type="html" xml:base="https://ggluo.github.io/blog/learn-c-5"><![CDATA[<p>I got the error message below when compiling my project. At the beginning, I had no idea what was going on. I only define the function <code class="language-plaintext highlighter-rouge">load_engine</code> once in the header file, and there is no definition elsewhere.</p> <pre><code class="language-log">/usr/bin/ld: CMakeFiles/main.dir/src/trt.cpp.o: in function `load_engine()':
trt.cpp:(.text+0x131): multiple definition of `load_engine()'; 
CMakeFiles/main.dir/src/main.cpp.o:main.cpp:(.text+0x0): first defined here
</code></pre> <p>It looks like this in the header file</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">struct</span> <span class="nc">infer_params</span>
<span class="p">{</span>
    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">weights_file</span><span class="p">;</span> <span class="c1">// currently supports only uff file</span>
    <span class="kt">int</span> <span class="n">batch_size</span><span class="p">;</span>
    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">save_engine</span><span class="p">;</span>
    <span class="n">std</span><span class="o">::</span><span class="n">string</span> <span class="n">load_engine</span><span class="p">;</span>
<span class="p">};</span>

<span class="kt">bool</span> <span class="nf">load_engine</span><span class="p">(){</span>
<span class="c1">// do something....</span>
<span class="p">}</span>
</code></pre></div></div> <p><code class="language-plaintext highlighter-rouge">load_engine</code> was defined with a string in the struct, and then <code class="language-plaintext highlighter-rouge">load_engine</code>was defined with a function. I could change the namings or use <code class="language-plaintext highlighter-rouge">inline</code> when define <code class="language-plaintext highlighter-rouge">load_engine</code> at the second time. As an inline function is a function defined in C++ that is expanded in place at each point in your code where it is called, rather than being called through the usual function call mechanism.</p>]]></content><author><name></name></author><category term="c++"/><summary type="html"><![CDATA[I got the error message below when compiling my project. At the beginning, I had no idea what was going on. I only define the function load_engine once in the header file, and there is no definition elsewhere.]]></summary></entry><entry><title type="html">Learn C++ 4: std::array vs std::vector</title><link href="https://ggluo.github.io/blog/learn-c-4" rel="alternate" type="text/html" title="Learn C++ 4: std::array vs std::vector"/><published>2023-03-09T18:28:00+00:00</published><updated>2023-03-09T18:28:00+00:00</updated><id>https://ggluo.github.io/blog/learn-c-4</id><content type="html" xml:base="https://ggluo.github.io/blog/learn-c-4"><![CDATA[<p><code class="language-plaintext highlighter-rouge">std::array</code> and <code class="language-plaintext highlighter-rouge">std::vector</code> are both container classes provided by the C++ Standard Library, but they have different characteristics and are used for different purposes:</p> <ol> <li><strong>Size</strong>: <ul> <li><strong><code class="language-plaintext highlighter-rouge">std::array</code></strong>: Has a fixed size determined at compile time, specified by its template parameter. Once created, the size of a <code class="language-plaintext highlighter-rouge">std::array</code> cannot be changed.</li> <li><strong><code class="language-plaintext highlighter-rouge">std::vector</code></strong>: Has a dynamic size that can grow or shrink at runtime. Elements can be added or removed from a <code class="language-plaintext highlighter-rouge">std::vector</code> after it’s created.</li> </ul> </li> <li><strong>Memory Allocation</strong>: <ul> <li><strong><code class="language-plaintext highlighter-rouge">std::array</code></strong>: Typically allocated on the stack. The size of a <code class="language-plaintext highlighter-rouge">std::array</code> must be known at compile time, so its memory is allocated statically.</li> <li><strong><code class="language-plaintext highlighter-rouge">std::vector</code></strong>: Typically allocated on the heap. The memory for a <code class="language-plaintext highlighter-rouge">std::vector</code> is dynamically allocated, allowing it to resize as needed.</li> </ul> </li> <li><strong>Accessing Elements</strong>: <ul> <li><strong><code class="language-plaintext highlighter-rouge">std::array</code></strong>: Supports constant time (O(1)) random access to elements using the <code class="language-plaintext highlighter-rouge">[]</code> operator. It provides efficient access to elements since the array is contiguous in memory.</li> <li><strong><code class="language-plaintext highlighter-rouge">std::vector</code></strong>: Supports constant time (O(1)) random access to elements using the <code class="language-plaintext highlighter-rouge">[]</code> operator. Like <code class="language-plaintext highlighter-rouge">std::array</code>, it provides efficient access to elements due to its contiguous memory layout.</li> </ul> </li> <li><strong>Performance</strong>: <ul> <li><strong><code class="language-plaintext highlighter-rouge">std::array</code></strong>: Typically offers better performance than <code class="language-plaintext highlighter-rouge">std::vector</code> for small, fixed-size collections because of its static memory allocation and lack of dynamic resizing overhead.</li> <li><strong><code class="language-plaintext highlighter-rouge">std::vector</code></strong>: Offers better performance for large collections or when the size of the collection is unknown at compile time, as it can dynamically resize itself to accommodate new elements.</li> </ul> </li> <li><strong>Usage</strong>: <ul> <li><strong><code class="language-plaintext highlighter-rouge">std::array</code></strong>: Useful when the size of the collection is known at compile time and does not change. It’s often used for small, fixed-size collections where the size is known in advance.</li> <li><strong><code class="language-plaintext highlighter-rouge">std::vector</code></strong>: Suitable when the size of the collection can change dynamically at runtime or when the size is not known at compile time. It’s a versatile container that can be resized and is widely used in C++.</li> </ul> </li> </ol> <p>In summary, choose <code class="language-plaintext highlighter-rouge">std::array</code> when you need a fixed-size container with known size at compile time, and choose <code class="language-plaintext highlighter-rouge">std::vector</code> when you need a dynamic-size container or when the size is not known at compile time.</p>]]></content><author><name></name></author><category term="c++"/><summary type="html"><![CDATA[std::array and std::vector are both container classes provided by the C++ Standard Library, but they have different characteristics and are used for different purposes:]]></summary></entry><entry><title type="html">How Bindings Work with I/O in TensorRT</title><link href="https://ggluo.github.io/blog/tensorrt-bindings" rel="alternate" type="text/html" title="How Bindings Work with I/O in TensorRT"/><published>2023-03-08T11:31:00+00:00</published><updated>2023-03-08T11:31:00+00:00</updated><id>https://ggluo.github.io/blog/bindings-in-tensorrt</id><content type="html" xml:base="https://ggluo.github.io/blog/tensorrt-bindings"><![CDATA[<p>In TensorRT, a “binding” refers to the association between a tensor in the computational graph (network) and a specific memory buffer that holds the data for that tensor during inference. Bindings are used to specify how input and output tensors of a neural network are mapped to memory buffers where the input data is provided and where the output data is retrieved after inference.</p> <p>Here’s a basic overview of how bindings work in TensorRT from the docs from <a href="https://docs.nvidia.com/deeplearning/tensorrt/api/python_api/infer/Core/Engine.html?highlight=binding#tensorrt.ICudaEngine">TensorRT engine</a> :</p> <ol> <li> <p><strong>Input Bindings</strong>: Input bindings specify where the input data for the neural network will be located during inference. Typically, this involves specifying memory buffers where the input data is stored, such as CPU or GPU memory.</p> </li> <li> <p><strong>Output Bindings</strong>: Output bindings specify where the output data produced by the neural network during inference will be placed. Similar to input bindings, output bindings specify memory buffers where the output data will be stored, such as CPU or GPU memory.</p> </li> <li> <p><strong>Binding Indices</strong>: Each input and output tensor in the network is associated with a unique binding index. These binding indices are used to identify which input or output tensor corresponds to which memory buffer during inference.</p> </li> <li> <p><strong>Binding Shapes</strong>: Along with specifying the memory location for input and output tensors, bindings also specify the shape (dimensions) of each tensor. This ensures that the memory buffers provided for input and output data have the correct size to accommodate the tensors produced by the network.</p> </li> <li> <p><strong>Binding Data Types</strong>: Bindings also specify the data type of each tensor, such as float32, int8, etc. This ensures that the memory buffers provided for input and output data have the correct data type to match the tensors produced by the network.</p> </li> </ol>]]></content><author><name></name></author><category term="c++"/><category term="tensorRT"/><summary type="html"><![CDATA[In TensorRT, a “binding” refers to the association between a tensor in the computational graph (network) and a specific memory buffer that holds the data for that tensor during inference. Bindings are used to specify how input and output tensors of a neural network are mapped to memory buffers where the input data is provided and where the output data is retrieved after inference.]]></summary></entry><entry><title type="html">Learn C++ 3: Disallow C++ class copy and assignment</title><link href="https://ggluo.github.io/blog/learn-c-3" rel="alternate" type="text/html" title="Learn C++ 3: Disallow C++ class copy and assignment"/><published>2023-03-02T12:31:00+00:00</published><updated>2023-03-02T12:31:00+00:00</updated><id>https://ggluo.github.io/blog/learn-c-3</id><content type="html" xml:base="https://ggluo.github.io/blog/learn-c-3"><![CDATA[<p>I came across the two lines below. At the beginning, I thought they are declaration for a customized destructor. It turned out that I’m wrong after a discussion with my colleague.</p> <div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">net_infer</span><span class="p">(</span><span class="k">const</span> <span class="n">net_infer</span><span class="o">&amp;</span><span class="p">)</span> <span class="o">=</span> <span class="k">delete</span><span class="p">;</span>
<span class="n">net_infer</span><span class="o">&amp;</span> <span class="k">operator</span><span class="o">=</span><span class="p">(</span><span class="k">const</span> <span class="n">net_infer</span><span class="o">&amp;</span><span class="p">)</span> <span class="o">=</span> <span class="k">delete</span><span class="p">;</span>
</code></pre></div></div> <h4 id="1-disallow-copy">1. Disallow copy</h4> <p>a. <strong>net_infer</strong>: This is the name of the class.</p> <p>b. <strong>const net_infer&amp;</strong>: This is a reference to a const object of type <strong>net_infer</strong>. It’s the parameter type of the copy constructor.</p> <p>c. <strong>= delete</strong>: This part of the declaration explicitly deletes the copy constructor. It means that attempts to create copies of <strong>net_infer</strong> objects using the copy constructor will result in a compilation error.</p> <h4 id="2-disallow-assignment">2. Disallow assignment</h4> <p>a. <strong>net_infer&amp;</strong>: This specifies the return type of the copy assignment operator. It returns a reference to a net_infer object.</p> <p>b. <strong>operator=</strong>: This is the name of the copy assignment operator.</p> <p>c. <strong>(const net_infer&amp;)</strong>: This specifies the parameter of the copy assignment operator, which is a reference to a const object of type net_infer. This parameter represents the object that is being assigned from.</p> <p>d. <strong>= delete</strong>: This part of the declaration explicitly deletes the copy assignment operator. It means that attempts to assign one net_infer object to another using the copy assignment operator will result in a compilation error.</p> <p>By deleting the copy assignment operator or the copy constructor, the class net_infer effectively disallows assignment operations and copy between its instances. This is useful in cases where copying or assigning instances of the class would lead to undesirable behavior, such as resource leaks or invalid state changes.</p>]]></content><author><name></name></author><category term="c++"/><summary type="html"><![CDATA[I came across the two lines below. At the beginning, I thought they are declaration for a customized destructor. It turned out that I’m wrong after a discussion with my colleague.]]></summary></entry><entry><title type="html">Learn C++ 2: Integrating TensorRT Libraries into CMake Projects</title><link href="https://ggluo.github.io/blog/learn-c-2" rel="alternate" type="text/html" title="Learn C++ 2: Integrating TensorRT Libraries into CMake Projects"/><published>2023-02-08T17:31:00+00:00</published><updated>2023-02-08T17:31:00+00:00</updated><id>https://ggluo.github.io/blog/learn-c-2</id><content type="html" xml:base="https://ggluo.github.io/blog/learn-c-2"><![CDATA[<p>I downloaded the tar file for <a href="https://docs.nvidia.com/deeplearning/tensorrt/install-guide/index.html#installing-tar"><code class="language-plaintext highlighter-rouge">TensorRT</code></a> local installation and unpacked it into a local folder. I want to use the dynamic libraries in it such as <code class="language-plaintext highlighter-rouge">libnvinfer.so</code> when compiling my project with CMake. How shoud I configurate it in the CMakeLists.txt?</p> <ol> <li>Locate the directory into which you unpack TensorRT tar file. This directory contains the header files (‘include/*.h’) and libraries (‘lib/*.so’). <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>set(TENSORRT_DIR /path/to/tensorrt)
</code></pre></div> </div> </li> <li>Use <code class="language-plaintext highlighter-rouge">find_library</code> command to find (‘*.so’) under directory <code class="language-plaintext highlighter-rouge">lib</code> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>find_library(NVINFER_LIB libnvinfer.so PATHS ${TENSORRT_DIR}/lib)
</code></pre></div> </div> </li> <li>Use <code class="language-plaintext highlighter-rouge">include_directories</code> command to include header files (‘*.h’) under directory <code class="language-plaintext highlighter-rouge">include</code> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>include_directories(${TENSORRT_DIR}/include)
</code></pre></div> </div> </li> <li>Use <code class="language-plaintext highlighter-rouge">link_libraries</code> command to link the TensorRT libraries to your target <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>link_libraries(${NVINFER_LIB} ${NVONNXPARSER_LIB} ${CUDA_LIBRARIES})
</code></pre></div> </div> </li> </ol> <p>The code block below is the complete CMake file.</p> <div class="language-cmake highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cmake_minimum_required</span><span class="p">(</span>VERSION 3.11<span class="p">)</span>
<span class="nb">project</span><span class="p">(</span>trt_examples<span class="p">)</span>
<span class="nb">set</span><span class="p">(</span>CMAKE_CXX_STANDARD 14<span class="p">)</span>

<span class="c1"># CUDA</span>
<span class="nb">find_package</span><span class="p">(</span>CUDA REQUIRED<span class="p">)</span>
<span class="nb">include_directories</span><span class="p">(</span><span class="si">${</span><span class="nv">CUDA_INCLUDE_DIRS</span><span class="si">}</span><span class="p">)</span>
<span class="nb">message</span><span class="p">(</span><span class="s2">"CUDA_TOOLKIT_ROOT_DIR = </span><span class="si">${</span><span class="nv">CUDA_TOOLKIT_ROOT_DIR</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">message</span><span class="p">(</span><span class="s2">"CUDA_INCLUDE_DIRS = </span><span class="si">${</span><span class="nv">CUDA_INCLUDE_DIRS</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>
<span class="nb">message</span><span class="p">(</span><span class="s2">"CUDA_LIBRARIES = </span><span class="si">${</span><span class="nv">CUDA_LIBRARIES</span><span class="si">}</span><span class="s2">"</span><span class="p">)</span>

<span class="c1"># TensorRT</span>
<span class="nb">set</span><span class="p">(</span>TENSORRT_DIR  <span class="s2">"/home/gluo/local_lib/TensorRT-8.6.1.6/"</span><span class="p">)</span>
<span class="nb">find_library</span><span class="p">(</span>NVINFER_LIB libnvinfer.so PATHS <span class="si">${</span><span class="nv">TENSORRT_DIR</span><span class="si">}</span>/lib<span class="p">)</span>
<span class="nb">find_library</span><span class="p">(</span>NVONNXPARSER_LIB libnvonnxparser.so PATHS <span class="si">${</span><span class="nv">TENSORRT_DIR</span><span class="si">}</span>/lib<span class="p">)</span>

<span class="nb">include_directories</span><span class="p">(</span><span class="si">${</span><span class="nv">TENSORRT_DIR</span><span class="si">}</span>/include<span class="p">)</span>
<span class="nb">link_libraries</span><span class="p">(</span><span class="si">${</span><span class="nv">NVINFER_LIB</span><span class="si">}</span> <span class="si">${</span><span class="nv">NVONNXPARSER_LIB</span><span class="si">}</span> <span class="si">${</span><span class="nv">CUDA_LIBRARIES</span><span class="si">}</span><span class="p">)</span>

<span class="nb">add_executable</span><span class="p">(</span>example1 example1.cpp<span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="c++"/><category term="tensorRT"/><summary type="html"><![CDATA[I downloaded the tar file for TensorRT local installation and unpacked it into a local folder. I want to use the dynamic libraries in it such as libnvinfer.so when compiling my project with CMake. How shoud I configurate it in the CMakeLists.txt?]]></summary></entry></feed>